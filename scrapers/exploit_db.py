"""
Exploit-DB scraper using web scraping (no official API available)
Website: https://www.exploit-db.com/
Enhanced with industry-level disclosure format parsing
"""

import trafilatura
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from .base import BaseScraper
from .disclosure_formats import DisclosureFormatManager, VulnerabilityDisclosure
import logging
import re

logger = logging.getLogger(__name__)

class ExploitDBScraper(BaseScraper):
    """Scraper for Exploit-DB (no official API, using web scraping)"""
    
    def __init__(self, config):
        super().__init__(config, 'exploit_db')
        self.base_url = "https://www.exploit-db.com"
        self.disclosure_manager = DisclosureFormatManager()
        
        # Be respectful with rate limiting for web scraping
        self.rate_limit_delay = 2.0
    
    async def scrape(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Scrape recent exploits from Exploit-DB"""
        vulnerabilities = []
        
        try:
            # Get recent exploits from the search page
            params = {
                'type': 'remote',  # Focus on remote exploits
                'page': 1
            }
            
            page = 1
            max_pages = 5  # Limit to first 5 pages to be respectful
            
            while page <= max_pages:
                params['page'] = page
                
                logger.info(f"Scraping Exploit-DB page {page}")
                
                response = await self.make_request(
                    url=f"{self.base_url}/search",
                    params=params
                )
                
                if not response or 'text' not in response:
                    logger.error("Failed to get response from Exploit-DB")
                    break
                
                html_content = response['text']
                exploits = self._parse_search_page(html_content)
                
                if not exploits:
                    logger.info("No more exploits found")
                    break
                
                logger.info(f"Processing {len(exploits)} exploits from Exploit-DB page {page}")
                
                for exploit_data in exploits:
                    try:
                        # Get detailed exploit information
                        detailed_exploit = await self._get_exploit_details(exploit_data)
                        if detailed_exploit:
                            # Parse using disclosure format manager
                            disclosure = self.disclosure_manager.parse_disclosure('exploit_db', detailed_exploit)
                            if disclosure:
                                # Convert to legacy format for compatibility
                                vuln = self._disclosure_to_dict(disclosure)
                                vulnerabilities.append(vuln)
                            
                            # Check limit
                            if limit and len(vulnerabilities) >= limit:
                                logger.info(f"Reached limit of {limit} vulnerabilities")
                                return vulnerabilities
                                
                    except Exception as e:
                        logger.error(f"Error processing exploit: {e}")
                        continue
                
                page += 1
            
            logger.info(f"Scraped {len(vulnerabilities)} vulnerabilities from Exploit-DB")
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"Error scraping Exploit-DB: {e}")
            return []
        finally:
            await self.close_session()
    
    def _parse_search_page(self, html_content: str) -> List[Dict[str, Any]]:
        """Parse the search results page to extract exploit information"""
        exploits = []
        
        try:
            # Use trafilatura to extract clean text
            clean_text = trafilatura.extract(html_content)
            if not clean_text:
                return exploits
            
            # Look for exploit entries in the HTML
            # Exploit-DB uses a table format with specific patterns
            exploit_pattern = r'href="/exploits/(\d+)"[^>]*>([^<]+)</a>'
            matches = re.findall(exploit_pattern, html_content)
            
            for exploit_id, title in matches:
                # Look for additional information in the surrounding HTML
                date_pattern = rf'exploits/{exploit_id}.*?(\d{{4}}-\d{{2}}-\d{{2}})'
                date_match = re.search(date_pattern, html_content)
                exploit_date = date_match.group(1) if date_match else None
                
                # Look for platform/type information
                platform_pattern = rf'exploits/{exploit_id}.*?(?:platform|type)[^>]*>([^<]+)</.*?'
                platform_match = re.search(platform_pattern, html_content, re.IGNORECASE)
                platform = platform_match.group(1) if platform_match else 'Unknown'
                
                exploits.append({
                    'id': exploit_id,
                    'title': title.strip(),
                    'date': exploit_date,
                    'platform': platform.strip(),
                    'url': f"{self.base_url}/exploits/{exploit_id}"
                })
                
                # Limit to avoid too many requests
                if len(exploits) >= 20:
                    break
            
        except Exception as e:
            logger.error(f"Error parsing Exploit-DB search page: {e}")
        
        return exploits
    
    async def _get_exploit_details(self, exploit_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Get detailed information about a specific exploit"""
        try:
            exploit_id = exploit_data['id']
            exploit_url = exploit_data['url']
            
            # Get the exploit details page
            response = await self.make_request(url=exploit_url)
            
            if not response or 'text' not in response:
                return None
            
            html_content = response['text']
            
            # Extract clean text content
            clean_text = trafilatura.extract(html_content)
            if not clean_text:
                return None
            
            # Parse the exploit details
            vuln = self._parse_exploit_details(exploit_data, html_content, clean_text)
            return vuln
            
        except Exception as e:
            logger.error(f"Error getting exploit details for {exploit_data.get('id', 'unknown')}: {e}")
            return None
    
    def _parse_exploit_details(self, exploit_data: Dict[str, Any], 
                              html_content: str, clean_text: str) -> Optional[Dict[str, Any]]:
        """Parse detailed exploit information"""
        try:
            title = exploit_data['title']
            exploit_id = exploit_data['id']
            exploit_url = exploit_data['url']
            
            # Extract CVE IDs from title and content
            cve_ids = self.extract_cve_ids(title + ' ' + clean_text)
            cve_id = cve_ids[0] if cve_ids else None
            
            # Create description from clean text
            description = title
            if clean_text:
                # Take first few lines as description
                lines = clean_text.split('\n')
                desc_lines = []
                for line in lines[:10]:  # First 10 lines
                    line = line.strip()
                    if line and len(line) > 10:
                        desc_lines.append(line)
                        if len(desc_lines) >= 3:  # Limit to 3 meaningful lines
                            break
                
                if desc_lines:
                    description += '\n\n' + '\n'.join(desc_lines)
            
            # Extract platform/technology information
            platform = exploit_data.get('platform', 'Unknown')
            
            # Look for additional metadata in HTML
            author_pattern = r'Author[:\s]*([^<\n]+)'
            author_match = re.search(author_pattern, html_content, re.IGNORECASE)
            author = author_match.group(1).strip() if author_match else None
            
            # Look for vulnerability type
            vuln_type = None
            type_patterns = [
                r'(?:Type|Category)[:\s]*([^<\n]+)',
                r'(?:SQL Injection|XSS|Buffer Overflow|RCE|Command Injection)'
            ]
            
            for pattern in type_patterns:
                type_match = re.search(pattern, html_content, re.IGNORECASE)
                if type_match:
                    vuln_type = type_match.group(1).strip() if type_match.groups() else type_match.group(0)
                    break
            
            # Determine severity (exploits are typically high severity)
            severity = 'high'
            if any(keyword in title.lower() for keyword in ['critical', 'remote code execution', 'rce']):
                severity = 'critical'
            elif any(keyword in title.lower() for keyword in ['dos', 'denial of service']):
                severity = 'medium'
            
            # Build affected products list
            affected_products = []
            if platform and platform != 'Unknown':
                affected_products.append(platform)
            
            # Extract product names from title
            # Common patterns: "ProductName Version - ExploitType"
            product_match = re.match(r'^([^-]+)', title)
            if product_match:
                product_name = product_match.group(1).strip()
                if product_name and product_name not in affected_products:
                    affected_products.append(product_name)
            
            # Build tags
            tags = ['exploit_db', 'exploit_available']
            if author:
                tags.append(f'author_{author.lower().replace(" ", "_")}')
            if vuln_type:
                tags.append(vuln_type.lower().replace(' ', '_'))
            if platform:
                tags.append(platform.lower().replace(' ', '_'))
            
            return self.create_vulnerability_dict(
                cve_id=cve_id,
                vulnerability_id=f"EDB-{exploit_id}",
                title=title,
                description=description,
                severity=severity,
                affected_products=affected_products,
                references=[exploit_url],
                exploit_available=True,  # All Exploit-DB entries have exploits
                poc_available=True,
                published_date=exploit_data.get('date'),
                source_url=exploit_url,
                vendor_response=author,
                tags=tags,
                raw_data=exploit_data
            )
            
        except Exception as e:
            logger.error(f"Error parsing exploit details {exploit_data.get('id', 'unknown')}: {e}")
            return None
    
    def _disclosure_to_dict(self, disclosure: VulnerabilityDisclosure) -> Dict[str, Any]:
        """Convert disclosure format to legacy vulnerability dictionary"""
        description = disclosure.description
        if disclosure.steps_to_reproduce:
            description += f"\n\nExploit Code:\n{disclosure.steps_to_reproduce}"
        if disclosure.impact:
            description += f"\n\nImpact:\n{disclosure.impact}"
        
        return self.create_vulnerability_dict(
            vulnerability_id=f"EDB-{disclosure.disclosure_id}",
            title=disclosure.title,
            description=description,
            severity=disclosure.severity,
            cve_id=disclosure.cve_id,
            affected_products=disclosure.affected_domains,
            references=[f"https://www.exploit-db.com/exploits/{disclosure.disclosure_id}"],
            published_date=disclosure.disclosure_date.isoformat() if disclosure.disclosure_date else None,
            source_url=f"https://www.exploit-db.com/exploits/{disclosure.disclosure_id}",
            tags=['exploit_db', 'exploit_available', disclosure.severity] + (
                [disclosure.vulnerability_type] if disclosure.vulnerability_type else []
            ),
            raw_data=disclosure.raw_data,
            exploit_available=True,  # All Exploit-DB entries have exploits
            poc_available=True,
            vendor_response=disclosure.researcher,  # Author field
            # Additional Exploit-DB-specific fields
            additional_data={
                'disclosure_format': 'exploit_db_v1',
                'author': disclosure.researcher,
                'vulnerability_type': disclosure.vulnerability_type,
                'affected_platforms': disclosure.affected_domains,
                'timeline': disclosure.timeline,
                'attachments': disclosure.attachments
            }
        )
    
    async def search_exploits(self, query: str) -> List[Dict[str, Any]]:
        """Search exploits by keyword"""
        try:
            params = {
                'search': query,
                'type': 'remote'
            }
            
            response = await self.make_request(
                url=f"{self.base_url}/search",
                params=params
            )
            
            if response and 'text' in response:
                exploits = self._parse_search_page(response['text'])
                vulnerabilities = []
                
                for exploit_data in exploits[:10]:  # Limit to first 10 results
                    vuln = await self._get_exploit_details(exploit_data)
                    if vuln:
                        vulnerabilities.append(vuln)
                
                return vulnerabilities
            
        except Exception as e:
            logger.error(f"Error searching Exploit-DB: {e}")
        
        return []
    
    async def get_exploit_by_id(self, exploit_id: str) -> Optional[Dict[str, Any]]:
        """Get a specific exploit by ID"""
        try:
            exploit_data = {
                'id': exploit_id,
                'title': f'Exploit {exploit_id}',
                'url': f"{self.base_url}/exploits/{exploit_id}"
            }
            
            return await self._get_exploit_details(exploit_data)
            
        except Exception as e:
            logger.error(f"Error getting exploit {exploit_id}: {e}")
        
        return None
